{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings, librosa\n",
        "from librosa.core import yin\n",
        "import numpy as np\n",
        "from time import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import utils\n",
        "from data import remove_accent\n",
        "from model import train_audio_transforms, AcousticModel, BoundaryDetection\n",
        "import json\n",
        "import argparse\n",
        "from lib import nets\n",
        "from lib import spec_utils\n",
        "from lib import dataset\n",
        "import json\n",
        "\n",
        "np.random.seed(7)\n",
        "\n",
        "new_rate = 22050\n",
        "\n",
        "def json2txt(json_path, save_path):\n",
        "  gt = open(json_path)\n",
        "  gt_json = json.load(gt)\n",
        "  words = []\n",
        "  with open(save_path, 'w', encoding=\"utf-8\") as f:\n",
        "    for line in gt_json:\n",
        "      words.extend([word['d'] for word in line['l']])\n",
        "      f.write(' '.join([word['d'] for word in line['l']])+' ')\n",
        "  return save_path\n",
        "\n",
        "def preprocess_from_file(audio_file, model_vocal, lyrics_file, word_file=None):\n",
        "    y, sr = preprocess_audio(audio_file, model_vocal)\n",
        "\n",
        "    words, lyrics_p, idx_word_p, idx_line_p = preprocess_lyrics(lyrics_file, word_file)\n",
        "\n",
        "    return y, words, lyrics_p, idx_word_p, idx_line_p\n",
        "\n",
        "def preprocess_audio(audio_file, model_vocal, sr=22050):\n",
        "    n_fft = 2048\n",
        "    hop_length = 1024\n",
        "    batchsize = 4\n",
        "    cropsize = 256\n",
        "    postprocess=True\n",
        "    device = \"cuda\"\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        X, curr_sr = librosa.load(audio_file, 44100, False, res_type='kaiser_fast')\n",
        "\n",
        "    if X.ndim == 1:\n",
        "        # mono to stereo\n",
        "        X = np.asarray([X, X])\n",
        "\n",
        "    X_spec = spec_utils.wave_to_spectrogram(X, hop_length, n_fft)\n",
        "    sp = Separator(model_vocal, device, batchsize, cropsize, postprocess)\n",
        "    _, v_spec = sp.separate_tta(X_spec)\n",
        "    wave = spec_utils.spectrogram_to_wave(v_spec, hop_length=hop_length)\n",
        "    y = librosa.resample(wave, orig_sr=44100, target_sr=22050)\n",
        "    y = librosa.to_mono(y)\n",
        "\n",
        "    if len(y.shape) == 1:\n",
        "        y = y[np.newaxis, :] # (channel, sample)\n",
        "\n",
        "    return y, curr_sr\n",
        "\n",
        "def preprocess_lyrics(lyrics_file, word_file=None):\n",
        "    from string import ascii_lowercase\n",
        "    # d = {ascii_lowercase[i]: i for i in range(26)}\n",
        "    # d[\"'\"] = 26\n",
        "    # d[\" \"] = 27\n",
        "    # d[\"~\"] = 28\n",
        "\n",
        "    # process raw\n",
        "    with open(lyrics_file, 'r') as f:\n",
        "        raw_lines = f.read().splitlines()\n",
        " \n",
        "    raw_lines = [\"\".join([remove_accent(c) for c in line.lower()]).strip() for line in raw_lines]\n",
        "    raw_lines = [\" \".join(line.split()) for line in raw_lines if len(line) > 0]\n",
        "    \n",
        "    # concat\n",
        "    full_lyrics = \" \".join(raw_lines)\n",
        "\n",
        "    if word_file:\n",
        "        with open(word_file) as f:\n",
        "            words_lines = f.read().splitlines()\n",
        "    else:\n",
        "        words_lines = full_lyrics.split()\n",
        "\n",
        "    lyrics_p, words_p, idx_word_p, idx_line_p = utils.gen_phone_gt(words_lines, raw_lines)\n",
        "\n",
        "    return words_lines, lyrics_p, idx_word_p, idx_line_p\n",
        "\n",
        "\n",
        "class Separator(object):\n",
        "\n",
        "    def __init__(self, model, device, batchsize, cropsize, postprocess=False):\n",
        "        self.model = model\n",
        "        self.offset = model.offset\n",
        "        self.device = device\n",
        "        self.batchsize = batchsize\n",
        "        self.cropsize = cropsize\n",
        "        self.postprocess = postprocess\n",
        "\n",
        "    def _separate(self, X_mag_pad, roi_size):\n",
        "        X_dataset = []\n",
        "        patches = (X_mag_pad.shape[2] - 2 * self.offset) // roi_size\n",
        "        for i in range(patches):\n",
        "            start = i * roi_size\n",
        "            X_mag_crop = X_mag_pad[:, :, start:start + self.cropsize]\n",
        "            X_dataset.append(X_mag_crop)\n",
        "\n",
        "        X_dataset = np.asarray(X_dataset)\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            mask = []\n",
        "            # To reduce the overhead, dataloader is not used.\n",
        "            for i in range(0, patches, self.batchsize):\n",
        "                X_batch = X_dataset[i: i + self.batchsize]\n",
        "                X_batch = torch.from_numpy(X_batch).to(self.device)\n",
        "\n",
        "                pred = self.model.predict_mask(X_batch)\n",
        "\n",
        "                pred = pred.detach().cpu().numpy()\n",
        "                pred = np.concatenate(pred, axis=2)\n",
        "                mask.append(pred)\n",
        "\n",
        "            mask = np.concatenate(mask, axis=2)\n",
        "\n",
        "        return mask\n",
        "\n",
        "    def _preprocess(self, X_spec):\n",
        "        X_mag = np.abs(X_spec)\n",
        "        X_phase = np.angle(X_spec)\n",
        "\n",
        "        return X_mag, X_phase\n",
        "\n",
        "    def _postprocess(self, mask, X_mag, X_phase):\n",
        "        if self.postprocess:\n",
        "            mask = spec_utils.merge_artifacts(mask)\n",
        "\n",
        "        y_spec = mask * X_mag * np.exp(1.j * X_phase)\n",
        "        v_spec = (1 - mask) * X_mag * np.exp(1.j * X_phase)\n",
        "\n",
        "        return y_spec, v_spec\n",
        "\n",
        "    def separate(self, X_spec):\n",
        "        X_mag, X_phase = self._preprocess(X_spec)\n",
        "\n",
        "        n_frame = X_mag.shape[2]\n",
        "        pad_l, pad_r, roi_size = dataset.make_padding(n_frame, self.cropsize, self.offset)\n",
        "        X_mag_pad = np.pad(X_mag, ((0, 0), (0, 0), (pad_l, pad_r)), mode='constant')\n",
        "        X_mag_pad /= X_mag_pad.max()\n",
        "\n",
        "        mask = self._separate(X_mag_pad, roi_size)\n",
        "        mask = mask[:, :, :n_frame]\n",
        "\n",
        "        y_spec, v_spec = self._postprocess(mask, X_mag, X_phase)\n",
        "\n",
        "        return y_spec, v_spec\n",
        "\n",
        "    def separate_tta(self, X_spec):\n",
        "        X_mag, X_phase = self._preprocess(X_spec)\n",
        "\n",
        "        n_frame = X_mag.shape[2]\n",
        "        pad_l, pad_r, roi_size = dataset.make_padding(n_frame, self.cropsize, self.offset)\n",
        "        X_mag_pad = np.pad(X_mag, ((0, 0), (0, 0), (pad_l, pad_r)), mode='constant')\n",
        "        X_mag_pad /= X_mag_pad.max()\n",
        "\n",
        "        mask = self._separate(X_mag_pad, roi_size)\n",
        "\n",
        "        pad_l += roi_size // 2\n",
        "        pad_r += roi_size // 2\n",
        "        X_mag_pad = np.pad(X_mag, ((0, 0), (0, 0), (pad_l, pad_r)), mode='constant')\n",
        "        X_mag_pad /= X_mag_pad.max()\n",
        "\n",
        "        mask_tta = self._separate(X_mag_pad, roi_size)\n",
        "        mask_tta = mask_tta[:, :, roi_size // 2:]\n",
        "        mask = (mask[:, :, :n_frame] + mask_tta[:, :, :n_frame]) * 0.5\n",
        "\n",
        "        y_spec, v_spec = self._postprocess(mask, X_mag, X_phase)\n",
        "\n",
        "        return y_spec, v_spec"
      ],
      "metadata": {
        "id": "3Hdx7c2c88a_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7mQaGFe4lWj"
      },
      "outputs": [],
      "source": [
        "ls_path_audio = \"./data/songs\"\n",
        "# ls_path_lyrics = args.ls_path_lyrics\n",
        "# ls_json_lyrics = args.ls_json_lyrics\n",
        "\n",
        "# convert json to txt\n",
        "ls_json_lyrics=\"./data/new_labels_json\"\n",
        "ls_path_lyrics=\"./data/new_labels_txt\"\n",
        "resolution = 256 / 22050 * 3\n",
        "\n",
        "ckp_path = \"/content/drive/MyDrive/Zalo_AI/duyhv6/LyricsAlignment-MTL/checkpoints/checkpoint_16\"\n",
        "save_folder = \"/content/results\"\n",
        "cuda = True\n",
        "method=\"Baseline\"\n",
        "n_fft = 2048\n",
        "\n",
        "# constants\n",
        "resolution = 256 / 22050 * 3\n",
        "alpha = 0.8\n",
        "\n",
        "# decode method\n",
        "if \"BDR\" in method:\n",
        "    model_type = method[:-4]\n",
        "    bdr_flag = True\n",
        "else:\n",
        "    model_type = method\n",
        "    bdr_flag = False\n",
        "bdr_flag = True\n",
        "print(\"Model: {} BDR?: {}\".format(model_type, bdr_flag))\n",
        "\n",
        "# prepare acoustic model params\n",
        "if model_type == \"Baseline\":\n",
        "    n_class = 41\n",
        "elif model_type == \"MTL\":\n",
        "    n_class = (41, 47)\n",
        "else:\n",
        "    ValueError(\"Invalid model type.\")\n",
        "\n",
        "hparams = {\n",
        "    \"n_cnn_layers\": 1,\n",
        "    \"n_rnn_layers\": 3,\n",
        "    \"rnn_dim\": 256,\n",
        "    \"n_class\": n_class,\n",
        "    \"n_feats\": 32,\n",
        "    \"stride\": 1,\n",
        "    \"dropout\": 0.1\n",
        "}\n",
        "\n",
        "device = 'cuda' if (cuda and torch.cuda.is_available()) else 'cpu'\n",
        "ac_model = AcousticModel(\n",
        "    hparams['n_cnn_layers'], hparams['rnn_dim'], hparams['n_class'], \\\n",
        "    hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        ").to(device)\n",
        "\n",
        "print(\"Loading remove vocals models...\")\n",
        "pretrained_model = \"./checkpoints/baseline.pth\"\n",
        "model_vocal = nets.CascadedNet(n_fft, 32, 128)\n",
        "model_vocal.load_state_dict(torch.load(pretrained_model, map_location=device))\n",
        "model_vocal.to(device)\n",
        "\n",
        "print(\"Loading acoustic model from checkpoint...\")\n",
        "state = utils.load_model(ac_model, ckp_path, cuda=(device==\"gpu\"))\n",
        "ac_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def write_time_file(all_predicted_time):\n",
        "  df = pd.DataFrame(all_predicted_time, columns =['File', \"Time\"])\n",
        "  if not os.path.exists(\"./result/\"):\n",
        "    os.mkdir(\"./result/\")\n",
        "  df.to_csv(\"./result/time_submission.csv\")\n",
        "\n",
        "  return "
      ],
      "metadata": {
        "id": "n_d64Tm6QCum"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls_path = os.listdir(ls_json_lyrics)\n",
        "all_predicted_time = []\n",
        "for i, path in enumerate(ls_path):\n",
        "      # start timer\n",
        "      t1 = time()\n",
        "      path = path.replace(\"json\", \"txt\")\n",
        "      if path == \"txt_lyrics\":\n",
        "        continue\n",
        "      audio_file =  os.path.join(ls_path_audio, path.replace(\".txt\", \".wav\"))\n",
        "      lyrics_file = os.path.join(ls_path_lyrics, path)\n",
        "      \n",
        "      audio, words, lyrics_p, idx_word_p, idx_line_p = preprocess_from_file(audio_file, model_vocal, lyrics_file, word_file=None)\n",
        "      # reshape input, prepare mel\n",
        "      x = audio.reshape(1, 1, -1)\n",
        "      x = utils.move_data_to_device(x, device)\n",
        "      x = x.squeeze(0)\n",
        "      x = x.squeeze(1)\n",
        "      x = train_audio_transforms.to(device)(x)\n",
        "      x = nn.utils.rnn.pad_sequence(x, batch_first=True).unsqueeze(1)\n",
        "      # predict\n",
        "      all_outputs = ac_model(x)\n",
        "      all_outputs = F.log_softmax(all_outputs, dim=2)\n",
        "\n",
        "      batch_num, output_length, num_classes = all_outputs.shape\n",
        "      song_pred = all_outputs.data.cpu().numpy().reshape(-1, num_classes)  # total_length, num_classes\n",
        "      total_length = int(audio.shape[1] / 22050 // resolution)\n",
        "      song_pred = song_pred[:total_length, :]\n",
        "\n",
        "      # smoothing\n",
        "      P_noise = np.random.uniform(low=1e-11, high=1e-10, size=song_pred.shape)\n",
        "      song_pred = np.log(np.exp(song_pred) + P_noise)\n",
        "\n",
        "      if bdr_flag:\n",
        "          # boundary model: fixed\n",
        "          bdr_hparams = {\n",
        "              \"n_cnn_layers\": 1,\n",
        "              \"rnn_dim\": 32,  # a smaller rnn dim than acoustic model\n",
        "              \"n_class\": 1,  # binary classification\n",
        "              \"n_feats\": 32,\n",
        "              \"stride\": 1,\n",
        "              \"dropout\": 0.1,\n",
        "          }\n",
        "\n",
        "          bdr_model = BoundaryDetection(\n",
        "              bdr_hparams['n_cnn_layers'], bdr_hparams['rnn_dim'], bdr_hparams['n_class'],\n",
        "              bdr_hparams['n_feats'], bdr_hparams['stride'], bdr_hparams['dropout']\n",
        "          ).to(device)\n",
        "\n",
        "          state = utils.load_model(bdr_model, \"./checkpoints/checkpoint_BDR\", cuda=(device == \"gpu\"))\n",
        "          bdr_model.eval()\n",
        "          # get boundary prob curve\n",
        "          bdr_outputs = bdr_model(x).data.cpu().numpy().reshape(-1)\n",
        "          # apply log\n",
        "          bdr_outputs = np.log(bdr_outputs) * alpha\n",
        "\n",
        "          line_start = [d[0] for d in idx_line_p]\n",
        "\n",
        "          # start alignment\n",
        "          word_align, score = utils.alignment_bdr(song_pred, lyrics_p, idx_word_p, bdr_outputs, line_start)\n",
        "      else:\n",
        "          # start alignment\n",
        "          word_align, score = utils.alignment(song_pred, lyrics_p, idx_word_p)\n",
        "\n",
        "      # Write json for submission\n",
        "      json_lyrics = json.load(open(os.path.join(ls_json_lyrics, path.replace(\".txt\", \".json\"))))\n",
        "      id = 0\n",
        "      lines_arr = []\n",
        "      spw = 0\n",
        "      for line in json_lyrics:\n",
        "        for word in line['l']:\n",
        "          try:\n",
        "            if remove_accent(word['d'].lower().strip()) == str(words[id]):\n",
        "              word['s'] = int(word_align[id][0]*1000*resolution)\n",
        "              word['e'] = int(word_align[id][1]*1000*resolution)\n",
        "              id += 1\n",
        "          except:\n",
        "            # print(remove_accent(word['d'].lower().strip().replace(\",\", '').replace('\"', \"\")))\n",
        "            continue\n",
        "        line['s'] = line[\"l\"][0][\"s\"]\n",
        "        line['e'] = line[\"l\"][-1][\"e\"]\n",
        "        spw += (line['e']-line['s'])/(len(line['l']))\n",
        "\n",
        "      spw = spw/len(json_lyrics)\n",
        "      if spw < 300:\n",
        "        for j, line in enumerate(json_lyrics):\n",
        "          for word in line['l']:\n",
        "            word['s'], word['e'] = max(word['s'] - 60, 0), max(word['e'] - 60, 0)\n",
        "          line['s'] = line['l'][0]['s']\n",
        "          line['e'] = line['l'][-1]['e']\n",
        "          lines_arr.append(line)\n",
        "          lines_arr = utils.fix_blank(lines_arr)\n",
        "      elif 300 <= spw <= 700:\n",
        "        for j, line in enumerate(json_lyrics):\n",
        "          for word in line['l']:\n",
        "            word['s'], word['e'] = max(word['s'] - 75, 0), max(word['e'] - 75, 0)\n",
        "          line['s'] = line['l'][0]['s']\n",
        "          line['e'] = line['l'][-1]['e']\n",
        "          lines_arr.append(line)\n",
        "          lines_arr = utils.fix_blank(lines_arr)\n",
        "      else:\n",
        "        for j, line in enumerate(json_lyrics):\n",
        "          for word in line['l']:\n",
        "            word['s'], word['e'] = max(word['s'] - 50, 0), max(word['e'] - 50, 0)\n",
        "          line['s'] = line['l'][0]['s']\n",
        "          line['e'] = line['l'][-1]['e']\n",
        "          lines_arr.append(line)\n",
        "          lines_arr = utils.fix_blank(lines_arr)\n",
        "      t2 = time()\n",
        "      predicted_time = int(t2*1000 - t1*1000)\n",
        "      all_predicted_time.append((path.replace(\".txt\", \"\"), predicted_time))\n",
        "      # Saving...\n",
        "      json_object = json.dumps(lines_arr, indent=4, ensure_ascii=False)\n",
        "      if not os.path.exists(save_folder):\n",
        "        os.mkdir(save_folder)\n",
        "      with open(os.path.join(save_folder, path.replace(\".txt\", \".json\")), \"w\", encoding=\"utf-8\") as outfile:\n",
        "          outfile.write(json_object)\n",
        "\n",
        "write_time_file(all_predicted_time)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzPeeeCg5q0H",
        "outputId": "715d4e80-d75a-478c-942b-0781124401c7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: [('37303837315f3134', 3059)]\n",
            "time: [('37303837315f3134', 3059), ('38303934305f3535', 2638)]\n",
            "time: [('37303837315f3134', 3059), ('38303934305f3535', 2638), ('37333437335f313032', 2378)]\n",
            "time: [('37303837315f3134', 3059), ('38303934305f3535', 2638), ('37333437335f313032', 2378), ('37363539395f3237', 5376)]\n",
            "time: [('37303837315f3134', 3059), ('38303934305f3535', 2638), ('37333437335f313032', 2378), ('37363539395f3237', 5376), ('37343438375f3832', 6453)]\n",
            "time: [('37303837315f3134', 3059), ('38303934305f3535', 2638), ('37333437335f313032', 2378), ('37363539395f3237', 5376), ('37343438375f3832', 6453), ('37353539325f3337', 12565)]\n"
          ]
        }
      ]
    }
  ]
}